{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-fy CORD-19 data\n",
    "\n",
    "Original code from https://www.kaggle.com/theamrzaki/covid-19-bert-researchpapers-semantic-search#Data-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "root_path = './../data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_json = glob.glob(f'{root_path}/**/*.json', recursive=True)\n",
    "\n",
    "len(all_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# get only rows with attached files\n",
    "\n",
    "metadata_path = f'{root_path}/metadata.csv'\n",
    "stripped_metadata_path = f'{root_path}/stripped_metadata.csv'\n",
    "\n",
    "if not os.path.exists(stripped_metadata_path):\n",
    "    meta_df = pd.read_csv(metadata_path, dtype={\n",
    "        'pubmed_id': str,\n",
    "        'Microsoft Academic Paper ID': str,\n",
    "        'doi': str\n",
    "    })\n",
    "\n",
    "    stripped_meta_df = meta_df.dropna(subset=['pmc_json_files'])\n",
    "\n",
    "    stripped_meta_df.to_csv(stripped_metadata_path)\n",
    "\n",
    "    stripped_meta_df.head()\n",
    "\n",
    "    del stripped_meta_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# create shuffled subset of metadata rows\n",
    "\n",
    "small_metadata_path = f'{root_path}/small/small_metadata.csv'\n",
    "if not os.path.exists(small_metadata_path):\n",
    "    small_metadata_file = open(f'{root_path}/small/small_metadata.csv', 'w')\n",
    "    # get header from metadata.csv\n",
    "    print(subprocess.run(\n",
    "        ['head', '-n 1', f'{root_path}/stripped_metadata.csv'], stdout=small_metadata_file))\n",
    "    #!head -n 1 \"{root_path}/stripped_metadata.csv\" > \"{root_path}/small_metadata.csv\"\n",
    "    # get random sample from metadata.csv\n",
    "    print(subprocess.run(\n",
    "        ['shuf', '-n 12500', f'{root_path}/stripped_metadata.csv'], stdout=small_metadata_file))\n",
    "    #!shuf -n 12500 \"{root_path}/stripped_metadata.csv\" >> \"{root_path}/small_metadata.csv\"\n",
    "    small_metadata_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = f'{root_path}/stripped_metadata.csv'\n",
    "\n",
    "meta_df = pd.read_csv(metadata_path, dtype={\n",
    "    'pubmed_id': str,\n",
    "    'Microsoft Academic Paper ID': str,\n",
    "    'doi': str\n",
    "})\n",
    "\n",
    "meta_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class Article:\n",
    "    def __init__(self, pmcid):\n",
    "\n",
    "        self.paper_id = ''\n",
    "        self.abstract = []\n",
    "        self.body_text = []\n",
    "\n",
    "        if not isinstance(pmcid, str) and math.isnan(pmcid):\n",
    "            return\n",
    "\n",
    "        with open(f\"{root_path}/document_parses/pmc_json/{pmcid}.xml.json\") as file:\n",
    "            content = json.load(file)\n",
    "            content_metadata = meta_df.loc[meta_df['pmcid'] == pmcid]\n",
    "\n",
    "            self.paper_id = content['paper_id']\n",
    "            self.abstract = []\n",
    "            self.body_text = []\n",
    "            self.metadata = {}\n",
    "\n",
    "            if not content_metadata is None:\n",
    "                self.metadata = content_metadata\n",
    "\n",
    "            if 'abstract' in content_metadata:\n",
    "                # Abstract\n",
    "                # self.abstract.append(content_metadata['abstract'][0])\n",
    "                for entry in content_metadata['abstract']:\n",
    "                    self.abstract.append(str(entry))\n",
    "                # print(self.abstract)\n",
    "            # Body text\n",
    "            if 'body_text' in content:\n",
    "                for entry in content['body_text']:\n",
    "                    self.body_text.append(entry['text'])\n",
    "\n",
    "            self.abstract = '\\n'.join(self.abstract)\n",
    "            self.body_text = '\\n'.join(self.body_text)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\n",
    "\n",
    "\n",
    "first_row = Article(meta_df['pmcid'][0])\n",
    "print(first_row.body_text)\n",
    "# meta_df.iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_breaks(content, length):\n",
    "    data = \"\"\n",
    "    words = content.split(' ')\n",
    "    total_chars = 0\n",
    "\n",
    "    # add break every length characters\n",
    "    for i in range(len(words)):\n",
    "        total_chars += len(words[i])\n",
    "        if total_chars > length:\n",
    "            data = data + \" <br> \" + words[i]\n",
    "            total_chars = 0\n",
    "        else:\n",
    "            data = data + \" \" + words[i]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import dataframe as dd\n",
    "\n",
    "global partial_df\n",
    "global dict_\n",
    "\n",
    "dict_ = {'paper_id': [], 'abstract': [], 'body_text': []}\n",
    "partial_df = pd.DataFrame(dict_, columns=[\n",
    "    'paper_id', 'abstract', 'body_text'])\n",
    "partial_df = dd.from_pandas(partial_df, npartitions=10)\n",
    "\n",
    "def populateDict(content):\n",
    "    # get metadata information\n",
    "    meta_data = content.metadata\n",
    "    # meta_df.loc[meta_df['pmcid'] == content.paper_id]\n",
    "    # no metadata, skip this paper\n",
    "    if len(meta_data) == 0:\n",
    "        return\n",
    "\n",
    "    # print(meta_data)\n",
    "\n",
    "    dict_['paper_id'].append(content.paper_id)\n",
    "    dict_['abstract'].append(content.abstract)\n",
    "    dict_['body_text'].append(content.body_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_df.compute().to_csv(f'{root_path}/df_covid.csv')\n",
    "\n",
    "\n",
    "def saveProgress():\n",
    "    global partial_df\n",
    "    global dict_\n",
    "\n",
    "    partial_df = pd.DataFrame(dict_, columns=[\n",
    "        'paper_id', 'abstract', 'body_text'])\n",
    "    partial_df = dd.from_pandas(partial_df, npartitions=32)\n",
    "\n",
    "    print('saving current progress')\n",
    "    partial_df.compute().to_csv(\n",
    "        f'{root_path}/df_covid.csv', mode='a', header=False)\n",
    "\n",
    "    print('reseting partial df')\n",
    "    del partial_df\n",
    "\n",
    "    print('reseting partial dict')\n",
    "    del dict_\n",
    "    dict_ = {'paper_id': [], 'abstract': [], 'body_text': []}\n",
    "\n",
    "\n",
    "for idx, entry in enumerate(meta_df['pmcid']):\n",
    "    if idx % (len(meta_df) // 100) == 0:\n",
    "        print(f'Processing index: {idx} of {len(meta_df)}')\n",
    "        saveProgress()\n",
    "\n",
    "    content = Article(entry)\n",
    "    populateDict(content)\n",
    "\n",
    "saveProgress()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import dataframe as dd\n",
    "import re\n",
    "\n",
    "\n",
    "def lower_case(input_str):\n",
    "    input_str = input_str.lower()\n",
    "    return input_str\n",
    "\n",
    "\n",
    "df_covid = dd.read_csv(f'{root_path}/df_covid.csv', sample=1000000)\n",
    "\n",
    "df_covid['body_text'] = df_covid['body_text'].astype(str)\n",
    "df_covid['abstract'] = df_covid['abstract'].astype(str)\n",
    "\n",
    "df_covid['abstract'] = df_covid['abstract'].apply(\n",
    "    lambda x: re.sub('[^a-zA-z0-9\\s]', '', x), meta=('abstract', 'str'))\n",
    "df_covid['body_text'] = df_covid['body_text'].apply(\n",
    "    lambda x: re.sub('[^a-zA-z0-9\\s]', '', x), meta=('body_text', 'str'))\n",
    "\n",
    "df_covid['abstract'] = df_covid['abstract'].apply(\n",
    "    lambda x: lower_case(x), meta=('abstract', 'str'))\n",
    "df_covid['body_text'] = df_covid['body_text'].apply(\n",
    "    lambda x: lower_case(x), meta=('body_text', 'str'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.to_csv(f'{root_path}/df_covid_preprocessed.csv', single_file=True, compute=True)\n",
    "\n",
    "df_covid.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "df_covid = dd.read_csv(f'{root_path}/df_covid_preprocessed.csv')\n",
    "\n",
    "df_covid = df_covid.drop(\n",
    "    [\"authors\", \"journal\", \"Unnamed: 0\", \"Unnamed: 0.1\"], axis=1)\n",
    "\n",
    "\n",
    "df_covid['body_text'] = df_covid['body_text'].astype(str)\n",
    "df_covid['abstract'] = df_covid['abstract'].astype(str)\n",
    "\n",
    "df_covid.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid['body_text'] = df_covid['body_text'].apply(\n",
    "    lambda x: x.split('\\n'), meta=('abstract', 'str'))\n",
    "\n",
    "df_covid = df_covid.explode('body_text')\n",
    "\n",
    "df_covid.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.to_csv(f'{root_path}/covid_sentences.csv', single_file=True, compute=True)\n",
    "\n",
    "df_covid.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_dict = text.to_dict()\n",
    "# len_text = len(text_dict[\"paper_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper_id_list = []\n",
    "# body_text_list = []\n",
    "\n",
    "# title_list = []\n",
    "# abstract_list = []\n",
    "# abstract_summary_list = []\n",
    "# for i in tqdm(range(0, len_text)):\n",
    "#     paper_id = text_dict[\"paper_id\"][i]\n",
    "#     body_text = text_dict[\"body_text\"][i].split(\"<br>\")\n",
    "#     title = text_dict[\"title\"][i]\n",
    "#     abstract = text_dict[\"abstract\"][i]\n",
    "#     abstract_summary = text_dict[\"abstract_summary\"][i]\n",
    "#     for b in body_text:\n",
    "#         paper_id_list.append(paper_id)\n",
    "#         body_text_list.append(b)\n",
    "#         title_list.append(title)\n",
    "#         abstract_list.append(abstract)\n",
    "#         abstract_summary_list.append(abstract_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sentences = pd.DataFrame({\"paper_id\": paper_id_list}, index=body_text_list)\n",
    "# df_sentences.to_csv(f'{root_path}/covid_sentences_body.csv')\n",
    "# df_sentences.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask import dataframe as dd\n",
    "\n",
    "# df_sentences = pd.DataFrame({\"paper_id\": paper_id_list, \"title\": title_list,\n",
    "#                             \"abstract\": abstract_list, \"abstract_summary\": abstract_summary_list}, index=body_text_list)\n",
    "# df_sentences = dd.from_pandas(df_sentences)\n",
    "# df_sentences.to_csv(f'{root_path}/covid_sentences.csv')\n",
    "# df_sentences.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import dataframe as dd\n",
    "\n",
    "df_sentences = dd.read_csv(f'{root_path}/covid_sentences.csv', blocksize=32e6)\n",
    "\n",
    "df_sentences.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sentences = df_sentences.set_index(\"Unnamed: 0\")\n",
    "\n",
    "# df_sentences.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_covid.to_csv(f'{root_path}/covid_sentences.csv', single_file=True, compute=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sentences = df_sentences[\"paper_id\"].to_dict()\n",
    "# df_sentences_list = list(df_sentences.keys())\n",
    "# len(df_sentences_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df_sentences.keys())[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sentences_list = [str(d) for d in tqdm(df_sentences_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/UKPLab/sentence-transformers/blob/master/examples/application_semantic_search.py\n",
    "\"\"\"\n",
    "This is a simple application for sentence embeddings: semantic search\n",
    "We have a corpus with various sentences. Then, for a given query sentence,\n",
    "we want to find the most similar sentence in this corpus.\n",
    "This script outputs for various queries the top 5 most similar sentences in the corpus.\n",
    "\"\"\"\n",
    "\n",
    "import pickle as pkl\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# embedder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "embedder = SentenceTransformer(f'{root_path}/models/pretrained/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedder.save(f'{root_path}/models/pretrained/', 'multi-qa-MiniLM-L6-cos-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Corpus with example sentences\n",
    "corpus = df_sentences['body_text']\n",
    "\n",
    "if not os.path.exists(f'{root_path}/pickles/corpus_embeddings.npy'):\n",
    "    corpus_embeddings = embedder.encode(\n",
    "        corpus, device='cuda', show_progress_bar=True)\n",
    "\n",
    "    # with open(f'{root_path}/pickles/corpus_embeddings.pkl', \"wb\") as file_:\n",
    "    #     pkl.dump(corpus_embeddings, file_)\n",
    "\n",
    "    torch.save(corpus_embeddings, f'{root_path}/pickles/corpus_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df = pd.read_csv(f'{root_path}/covid_sentences.csv', index_col=0)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df_sentences[['ind', 'body_text']]\n",
    "\n",
    "corpus.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = corpus.loc[corpus['ind'] == 0].compute()\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial\n",
    "import torch\n",
    "\n",
    "# with open(f'{root_path}/pickles/corpus_embeddings.pkl', \"rb\") as file_:\n",
    "#     corpus_embeddings = pkl.load(file_)\n",
    "corpus_embeddings = torch.load(f'{root_path}/pickles/small_corpus_embeddings.npy')\n",
    "\n",
    "# Query sentences:\n",
    "queries = ['What has been published about medical care?',\n",
    "           'Knowledge of the frequency, manifestations, and course of extrapulmonary manifestations of COVID-19, including, but not limited to, possible cardiomyopathy and cardiac arrest',\n",
    "           'Use of AI in real-time health care delivery to evaluate interventions, risk factors, and outcomes in a way that could not be done manually',\n",
    "           'Resources to support skilled nursing facilities and long term care facilities.',\n",
    "           'Mobilization of surge medical staff to address shortages in overwhelmed communities .',\n",
    "           'Age-adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS) with/without other organ failure â€“ particularly for viral etiologies .']\n",
    "query_embeddings = embedder.encode(queries, device='cuda', show_progress_bar=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_row(idx):\n",
    "    pass\n",
    "\n",
    "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "closest_n = 5\n",
    "print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "for query, query_embedding in zip(queries, query_embeddings):\n",
    "    distances = scipy.spatial.distance.cdist(\n",
    "        [query_embedding], corpus_embeddings, \"cosine\")[0]\n",
    "\n",
    "    results = zip(range(len(distances)), distances)\n",
    "    results = sorted(results, key=lambda x: x[1])\n",
    "\n",
    "    print(\"\\n\\n=========================================================\")\n",
    "    print(\"==========================Query==============================\")\n",
    "    print(\"===\", query, \"=====\")\n",
    "    print(\"=========================================================\")\n",
    "\n",
    "    for idx, distance in results[0:closest_n]:\n",
    "        print(\"Score:   \", \"(Score: %.4f)\" % (1-distance), \"\\n\")\n",
    "        print(\"Article Index:   \", idx, \"\\n\")\n",
    "        # print(\"Paragraph:   \", get_corpus_row(idx), \"\\n\")\n",
    "        # row_dict = df_sentences.loc[df_sentences.index ==\n",
    "        #                             corpus.loc[idx]].to_dict()\n",
    "        # print(\"paper_id:  \", row_dict[\"paper_id\"][corpus[idx]], \"\\n\")\n",
    "        # print(\"Title:  \", row_dict[\"title\"][corpus[idx]], \"\\n\")\n",
    "        # print(\"Abstract:  \", row_dict[\"abstract\"][corpus[idx]], \"\\n\")\n",
    "        # print(\"Abstract_Summary:  \",\n",
    "        #       row_dict[\"abstract_summary\"][corpus[idx]], \"\\n\")\n",
    "        print(\"-------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "torch.cuda.current_device()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2ea2f29b5b448882058d44cbab5bc411432e8b9448117a1eba11f607af628be"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('39venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
