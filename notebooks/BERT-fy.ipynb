{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-fy CORD-19 data\n",
    "\n",
    "Original code from https://www.kaggle.com/theamrzaki/covid-19-bert-researchpapers-semantic-search#Data-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "root_path = './../data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_json = glob.glob(f'{root_path}/**/*.json', recursive=True)\n",
    "\n",
    "len(all_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# get only rows with attached files\n",
    "\n",
    "metadata_path = f'{root_path}/metadata.csv'\n",
    "stripped_metadata_path = f'{root_path}/stripped_metadata.csv'\n",
    "\n",
    "if not os.path.exists(stripped_metadata_path):\n",
    "    meta_df = pd.read_csv(metadata_path, dtype={\n",
    "        'pubmed_id': str,\n",
    "        'Microsoft Academic Paper ID': str,\n",
    "        'doi': str\n",
    "    })\n",
    "\n",
    "    stripped_meta_df = meta_df.dropna(subset=['pmc_json_files'])\n",
    "\n",
    "    stripped_meta_df.to_csv(stripped_metadata_path)\n",
    "\n",
    "    stripped_meta_df.head()\n",
    "\n",
    "    del stripped_meta_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "small_metadata_path = f'{root_path}/small_metadata.csv'\n",
    "if not os.path.exists(small_metadata_path):\n",
    "    small_metadata_file = open(f'{root_path}/small_metadata.csv', 'w')\n",
    "    # get header from metadata.csv\n",
    "    print(subprocess.run(\n",
    "        ['head', '-n 1', f'{root_path}/stripped_metadata.csv'], stdout=small_metadata_file))\n",
    "    #!head -n 1 \"{root_path}/stripped_metadata.csv\" > \"{root_path}/small_metadata.csv\"\n",
    "    # get random sample from metadata.csv\n",
    "    print(subprocess.run(\n",
    "        ['shuf', '-n 12500', f'{root_path}/stripped_metadata.csv'], stdout=small_metadata_file))\n",
    "    #!shuf -n 12500 \"{root_path}/stripped_metadata.csv\" >> \"{root_path}/small_metadata.csv\"\n",
    "    small_metadata_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = f'{root_path}/stripped_metadata.csv'\n",
    "\n",
    "meta_df = pd.read_csv(metadata_path, dtype={\n",
    "    'pubmed_id': str,\n",
    "    'Microsoft Academic Paper ID': str,\n",
    "    'doi': str\n",
    "})\n",
    "\n",
    "meta_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class Article:\n",
    "    def __init__(self, pmcid):\n",
    "\n",
    "        self.paper_id = ''\n",
    "        self.abstract = []\n",
    "        self.body_text = []\n",
    "\n",
    "        if not isinstance(pmcid, str) and math.isnan(pmcid):\n",
    "            return\n",
    "\n",
    "        with open(f\"{root_path}/document_parses/pmc_json/{pmcid}.xml.json\") as file:\n",
    "            content = json.load(file)\n",
    "            content_metadata = meta_df.loc[meta_df['pmcid'] == pmcid]\n",
    "\n",
    "            self.paper_id = content['paper_id']\n",
    "            self.abstract = []\n",
    "            self.body_text = []\n",
    "            self.metadata = {}\n",
    "\n",
    "            if not content_metadata is None:\n",
    "                self.metadata = content_metadata\n",
    "\n",
    "            if 'abstract' in content_metadata:\n",
    "                # Abstract\n",
    "                # self.abstract.append(content_metadata['abstract'][0])\n",
    "                for entry in content_metadata['abstract']:\n",
    "                    self.abstract.append(str(entry))\n",
    "                # print(self.abstract)\n",
    "            # Body text\n",
    "            if 'body_text' in content:\n",
    "                for entry in content['body_text']:\n",
    "                    self.body_text.append(entry['text'])\n",
    "\n",
    "            self.abstract = '\\n'.join(self.abstract)\n",
    "            self.body_text = '\\n'.join(self.body_text)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\n",
    "\n",
    "\n",
    "first_row = Article(meta_df['pmcid'][0])\n",
    "print(first_row)\n",
    "# meta_df.iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_breaks(content, length):\n",
    "    data = \"\"\n",
    "    words = content.split(' ')\n",
    "    total_chars = 0\n",
    "\n",
    "    # add break every length characters\n",
    "    for i in range(len(words)):\n",
    "        total_chars += len(words[i])\n",
    "        if total_chars > length:\n",
    "            data = data + \"<br>\" + words[i]\n",
    "            total_chars = 0\n",
    "        else:\n",
    "            data = data + \" \" + words[i]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import dataframe as dd\n",
    "\n",
    "global partial_df\n",
    "global dict_\n",
    "\n",
    "dict_ = {'paper_id': [], 'abstract': [], 'body_text': [],\n",
    "         'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\n",
    "partial_df = pd.DataFrame(dict_, columns=[\n",
    "    'paper_id', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\n",
    "partial_df = dd.from_pandas(partial_df, npartitions=10)\n",
    "\n",
    "def populateDict(content):\n",
    "    # get metadata information\n",
    "    meta_data = content.metadata\n",
    "    # meta_df.loc[meta_df['pmcid'] == content.paper_id]\n",
    "    # no metadata, skip this paper\n",
    "    if len(meta_data) == 0:\n",
    "        return\n",
    "\n",
    "    # print(meta_data)\n",
    "\n",
    "    dict_['paper_id'].append(content.paper_id)\n",
    "    dict_['abstract'].append(content.abstract.replace('\\n', '<br>'))\n",
    "    dict_['body_text'].append(content.body_text.replace('\\n', '<br>'))\n",
    "\n",
    "    # also create a column for the summary of abstract to be used in a plot\n",
    "    if len(content.abstract) == 0:\n",
    "        # no abstract provided\n",
    "        dict_['abstract_summary'].append(\"Not provided.\")\n",
    "    elif len(content.abstract.split(' ')) > 100:\n",
    "        # abstract provided is too long for plot, take first 300 words append with ...\n",
    "        info = content.abstract.split(' ')[:100]\n",
    "        summary = get_breaks(' '.join(info), 40)\n",
    "        dict_['abstract_summary'].append(summary + \"...\")\n",
    "    else:\n",
    "        # abstract is short enough\n",
    "        summary = get_breaks(content.abstract, 40)\n",
    "        dict_['abstract_summary'].append(summary)\n",
    "\n",
    "    # get metadata information\n",
    "    # meta_data = meta_df.loc[meta_df['pmcid'] == content.paper_id]\n",
    "\n",
    "    try:\n",
    "        # if more than one author\n",
    "        authors = meta_data['authors'].values[0].split(';')\n",
    "        if len(authors) > 2:\n",
    "            # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n",
    "            dict_['authors'].append(\". \".join(authors[:2]) + \"...\")\n",
    "        else:\n",
    "            # authors will fit in plot\n",
    "            dict_['authors'].append(\". \".join(authors))\n",
    "    except Exception as e:\n",
    "        # if only one author - or Null valie\n",
    "        dict_['authors'].append(meta_data['authors'].values[0])\n",
    "\n",
    "    # add the title information, add breaks when needed\n",
    "    try:\n",
    "        title = get_breaks(meta_data['title'].values[0], 40)\n",
    "        dict_['title'].append(title)\n",
    "    # if title was not provided\n",
    "    except Exception as e:\n",
    "        dict_['title'].append(meta_data['title'].values[0])\n",
    "\n",
    "    # add the journal information\n",
    "    dict_['journal'].append(meta_data['journal'].values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_df.compute().to_csv(f'{root_path}/df_covid.csv')\n",
    "\n",
    "def saveProgress():\n",
    "    global partial_df\n",
    "    global dict_\n",
    "\n",
    "    partial_df = pd.DataFrame(dict_, columns=[\n",
    "        'paper_id', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\n",
    "    partial_df = dd.from_pandas(partial_df, npartitions=10)\n",
    "\n",
    "    print('saving current progress')\n",
    "    partial_df.compute().to_csv(f'{root_path}/df_covid.csv', mode='a', header=False)\n",
    "\n",
    "    print('reseting partial df')\n",
    "    del partial_df\n",
    "\n",
    "    print('reseting partial dict')\n",
    "    del dict_\n",
    "    dict_ = {'paper_id': [], 'abstract': [], 'body_text': [],\n",
    "             'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\n",
    "\n",
    "\n",
    "for idx, entry in enumerate(meta_df['pmcid']):\n",
    "    if idx % (len(meta_df) // 100) == 0:\n",
    "        print(f'Processing index: {idx} of {len(meta_df)}')\n",
    "        saveProgress()\n",
    "\n",
    "    content = Article(entry)\n",
    "    populateDict(content)\n",
    "\n",
    "saveProgress()\n",
    "\n",
    "# df_covid = pd.DataFrame(dict_, columns=[\n",
    "#                        'paper_id', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\n",
    "# df_covid.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import dataframe as dd\n",
    "import re\n",
    "\n",
    "\n",
    "def lower_case(input_str):\n",
    "    input_str = input_str.lower()\n",
    "    return input_str\n",
    "\n",
    "\n",
    "df_covid = dd.read_csv(f'{root_path}/df_covid.csv', sample=1000000)\n",
    "\n",
    "df_covid['body_text'] = df_covid['body_text'].astype(str)\n",
    "df_covid['abstract'] = df_covid['abstract'].astype(str)\n",
    "\n",
    "df_covid['abstract'] = df_covid['abstract'].apply(\n",
    "    lambda x: re.sub('[^a-zA-z0-9\\s]', '', x), meta=('abstract', 'str'))\n",
    "df_covid['body_text'] = df_covid['body_text'].apply(\n",
    "    lambda x: re.sub('[^a-zA-z0-9\\s]', '', x), meta=('body_text', 'str'))\n",
    "\n",
    "df_covid['abstract'] = df_covid['abstract'].apply(\n",
    "    lambda x: lower_case(x), meta=('abstract', 'str'))\n",
    "df_covid['body_text'] = df_covid['body_text'].apply(\n",
    "    lambda x: lower_case(x), meta=('body_text', 'str'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.to_csv(f'{root_path}/df_covid_preprocessed.csv', single_file=True, compute=True)\n",
    "\n",
    "df_covid.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# chunksize = 1000\n",
    "# chunk = None\n",
    "# with pd.read_csv(f'{root_path}/df_covid.csv', chunksize=chunksize) as reader:\n",
    "#     for chunk in reader:\n",
    "#         chunk['body_text'] = chunk['body_text'].apply(\n",
    "#             lambda x: re.sub('[^a-zA-z0-9\\s]', '', x))\n",
    "#         chunk['abstract'] = chunk['abstract'].apply(\n",
    "#             lambda x: re.sub('[^a-zA-z0-9\\s]', '', x))\n",
    "\n",
    "#         chunk['body_text'] = chunk['body_text'].apply(lambda x: lower_case(x))\n",
    "#         chunk['abstract'] = chunk['abstract'].apply(lambda x: lower_case(x))\n",
    "\n",
    "#         print('saving current progress')\n",
    "#         if chunk:\n",
    "#             chunk.to_csv(f'{root_path}/df_covid_processed.csv', mode='a', header=False)\n",
    "#         else:\n",
    "#             chunk.to_csv(f'{root_path}/df_covid_processed.csv')\n",
    "\n",
    "# df_covid.to_csv(f'{root_path}/df_covid.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import dataframe as dd\n",
    "\n",
    "df_covid = dd.read_csv(f'{root_path}/df_covid_preprocessed.csv')\n",
    "\n",
    "df_covid = df_covid.drop(\n",
    "    [\"authors\", \"journal\", \"Unnamed: 0\", \"Unnamed: 0.1\"], axis=1)\n",
    "\n",
    "\n",
    "df_covid['body_text'] = df_covid['body_text'].astype(str)\n",
    "df_covid['abstract'] = df_covid['abstract'].astype(str)\n",
    "\n",
    "df_covid.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid['body_text'] = df_covid['body_text'].apply(\n",
    "    lambda x: x.split('<br>'), meta=('abstract', 'str'))\n",
    "\n",
    "df_covid = df_covid.explode('body_text')\n",
    "\n",
    "df_covid.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.to_csv(f'{root_path}/covid_sentences.csv', single_file=True, compute=True)\n",
    "\n",
    "df_covid.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_dict = text.to_dict()\n",
    "# len_text = len(text_dict[\"paper_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper_id_list = []\n",
    "# body_text_list = []\n",
    "\n",
    "# title_list = []\n",
    "# abstract_list = []\n",
    "# abstract_summary_list = []\n",
    "# for i in tqdm(range(0, len_text)):\n",
    "#     paper_id = text_dict[\"paper_id\"][i]\n",
    "#     body_text = text_dict[\"body_text\"][i].split(\"<br>\")\n",
    "#     title = text_dict[\"title\"][i]\n",
    "#     abstract = text_dict[\"abstract\"][i]\n",
    "#     abstract_summary = text_dict[\"abstract_summary\"][i]\n",
    "#     for b in body_text:\n",
    "#         paper_id_list.append(paper_id)\n",
    "#         body_text_list.append(b)\n",
    "#         title_list.append(title)\n",
    "#         abstract_list.append(abstract)\n",
    "#         abstract_summary_list.append(abstract_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sentences = pd.DataFrame({\"paper_id\": paper_id_list}, index=body_text_list)\n",
    "# df_sentences.to_csv(f'{root_path}/covid_sentences_body.csv')\n",
    "# df_sentences.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask import dataframe as dd\n",
    "\n",
    "# df_sentences = pd.DataFrame({\"paper_id\": paper_id_list, \"title\": title_list,\n",
    "#                             \"abstract\": abstract_list, \"abstract_summary\": abstract_summary_list}, index=body_text_list)\n",
    "# df_sentences = dd.from_pandas(df_sentences)\n",
    "# df_sentences.to_csv(f'{root_path}/covid_sentences.csv')\n",
    "# df_sentences.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body_text</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract_summary</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PMC7169746</td>\n",
       "      <td>floodplain lakes of large river systems contai...</td>\n",
       "      <td>the white river is the largest river basin in ...</td>\n",
       "      <td>Relationships between Floodplain Lake Fish&lt;br...</td>\n",
       "      <td>Floodplain lakes of large river systems&lt;br&gt;co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PMC7649889</td>\n",
       "      <td>extreme heat events are now more frequent in m...</td>\n",
       "      <td>climate change is altering the earths global l...</td>\n",
       "      <td>Cardiovascular risks of climate change</td>\n",
       "      <td>Extreme heat events are now more frequent in&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PMC7845504</td>\n",
       "      <td>till date millions of people are infected by s...</td>\n",
       "      <td>the outbreak of covid 19 caused by severe acut...</td>\n",
       "      <td>Inhibitory efficiency of potential drugs&lt;br&gt;a...</td>\n",
       "      <td>Till date millions of people are infected by&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PMC8371718</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dear editorbrwe would like to share ideas on a...</td>\n",
       "      <td>Acute Myocardial Injury Following COVID-19&lt;br...</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PMC7773811</td>\n",
       "      <td>background the covid19 pandemic has resulted i...</td>\n",
       "      <td>given the epidemiological situation caused by ...</td>\n",
       "      <td>Emotional Impact of COVID-19 Lockdown Among&lt;b...</td>\n",
       "      <td>BACKGROUND: The COVID-19 pandemic has&lt;br&gt;resu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              paper_id                                           abstract  \\\n",
       "Unnamed: 0                                                                  \n",
       "0           PMC7169746  floodplain lakes of large river systems contai...   \n",
       "0           PMC7649889  extreme heat events are now more frequent in m...   \n",
       "0           PMC7845504  till date millions of people are infected by s...   \n",
       "0           PMC8371718                                                NaN   \n",
       "0           PMC7773811  background the covid19 pandemic has resulted i...   \n",
       "\n",
       "                                                    body_text  \\\n",
       "Unnamed: 0                                                      \n",
       "0           the white river is the largest river basin in ...   \n",
       "0           climate change is altering the earths global l...   \n",
       "0           the outbreak of covid 19 caused by severe acut...   \n",
       "0           dear editorbrwe would like to share ideas on a...   \n",
       "0           given the epidemiological situation caused by ...   \n",
       "\n",
       "                                                        title  \\\n",
       "Unnamed: 0                                                      \n",
       "0            Relationships between Floodplain Lake Fish<br...   \n",
       "0                      Cardiovascular risks of climate change   \n",
       "0            Inhibitory efficiency of potential drugs<br>a...   \n",
       "0            Acute Myocardial Injury Following COVID-19<br...   \n",
       "0            Emotional Impact of COVID-19 Lockdown Among<b...   \n",
       "\n",
       "                                             abstract_summary  \n",
       "Unnamed: 0                                                     \n",
       "0            Floodplain lakes of large river systems<br>co...  \n",
       "0            Extreme heat events are now more frequent in<...  \n",
       "0            Till date millions of people are infected by<...  \n",
       "0                                                         nan  \n",
       "0            BACKGROUND: The COVID-19 pandemic has<br>resu...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask import dataframe as dd\n",
    "\n",
    "df_sentences = dd.read_csv(f'{root_path}/covid_sentences.csv')\n",
    "df_sentences = df_sentences.set_index(\"Unnamed: 0\")\n",
    "\n",
    "df_sentences.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences = df_sentences[\"paper_id\"].to_dict()\n",
    "df_sentences_list = list(df_sentences.keys())\n",
    "len(df_sentences_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_sentences.keys())[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences_list = [str(d) for d in tqdm(df_sentences_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/UKPLab/sentence-transformers/blob/master/examples/application_semantic_search.py\n",
    "\"\"\"\n",
    "This is a simple application for sentence embeddings: semantic search\n",
    "We have a corpus with various sentences. Then, for a given query sentence,\n",
    "we want to find the most similar sentence in this corpus.\n",
    "This script outputs for various queries the top 5 most similar sentences in the corpus.\n",
    "\"\"\"\n",
    "\n",
    "import pickle as pkl\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedder = SentenceTransformer('bert-base-nli-mean-tokens')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus with example sentences\n",
    "corpus = df_covid['body_text']\n",
    "corpus_embeddings = embedder.encode(\n",
    "    corpus, device='cuda', show_progress_bar=True)\n",
    "\n",
    "with open(f'{root_path}/pickles/corpus_embeddings.pkl', \"wb\") as file_:\n",
    "    pkl.dump(corpus_embeddings, file_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df = pd.read_csv(f'{root_path}/covid_sentences.csv', index_col=0)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial\n",
    "with open(f'{root_path}/pickles/corpus_embeddings.pkl', \"rb\") as file_:\n",
    "    corpus_embeddings = pkl.load(file_)\n",
    "\n",
    "# Query sentences:\n",
    "queries = ['What has been published about medical care?',\n",
    "           'Knowledge of the frequency, manifestations, and course of extrapulmonary manifestations of COVID-19, including, but not limited to, possible cardiomyopathy and cardiac arrest',\n",
    "           'Use of AI in real-time health care delivery to evaluate interventions, risk factors, and outcomes in a way that could not be done manually',\n",
    "           'Resources to support skilled nursing facilities and long term care facilities.',\n",
    "           'Mobilization of surge medical staff to address shortages in overwhelmed communities .',\n",
    "           'Age-adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS) with/without other organ failure – particularly for viral etiologies .']\n",
    "query_embeddings = embedder.encode(queries, show_progress_bar=True)\n",
    "\n",
    "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "closest_n = 5\n",
    "print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "for query, query_embedding in zip(queries, query_embeddings):\n",
    "    distances = scipy.spatial.distance.cdist(\n",
    "        [query_embedding], corpus_embeddings, \"cosine\")[0]\n",
    "\n",
    "    results = zip(range(len(distances)), distances)\n",
    "    results = sorted(results, key=lambda x: x[1])\n",
    "\n",
    "    print(\"\\n\\n=========================================================\")\n",
    "    print(\"==========================Query==============================\")\n",
    "    print(\"===\", query, \"=====\")\n",
    "    print(\"=========================================================\")\n",
    "\n",
    "    for idx, distance in results[0:closest_n]:\n",
    "        print(\"Score:   \", \"(Score: %.4f)\" % (1-distance), \"\\n\")\n",
    "        print(\"Paragraph:   \", corpus[idx].strip(), \"\\n\")\n",
    "        row_dict = df_sentences.loc[df_sentences.index ==\n",
    "                                    corpus[idx]].to_dict()\n",
    "        print(\"paper_id:  \", row_dict[\"paper_id\"][corpus[idx]], \"\\n\")\n",
    "        print(\"Title:  \", row_dict[\"title\"][corpus[idx]], \"\\n\")\n",
    "        print(\"Abstract:  \", row_dict[\"abstract\"][corpus[idx]], \"\\n\")\n",
    "        print(\"Abstract_Summary:  \",\n",
    "              row_dict[\"abstract_summary\"][corpus[idx]], \"\\n\")\n",
    "        print(\"-------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "torch.cuda.current_device()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2ea2f29b5b448882058d44cbab5bc411432e8b9448117a1eba11f607af628be"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('39venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
